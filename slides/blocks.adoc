
Building blocks
===============
:author: Bela Ban belaban@yahoo.com
:backend: deckjs
:deckjs_transition: fade
:navigation:
:deckjs_theme: web-2.0
:deckjs_transition: fade
:goto:
:menu:
:toc:
:status:



Abstractions (over a channel)
-----------------------------
* Method invocations (RPCs) across a cluster
* Replicated and distributed hashmaps
* Distributed counters
* Distributed locks and
* Task execution across cluster nodes



Method invocations across a cluster
-----------------------------------

In this section we'll look at ways to invoke methods in cluster nodes, in blocking or non-blocking fashion

'''

RpcDispatcher
-------------
* Class `RpcDispatcher` (name derived from **R**emote **P**rocedure **C**alls) provides the capability of invoking
methods in a single node (unicast invocation), or in all nodes of a cluster (multicast invocation)
* Invocations can be _synchronous_ (blocking) or _non-synchronous_ (non-blocking)
* Any Java method can be invoked
* With multicast invocations, a `RspList<T>` object is returned:
[source,java]
----
public class RspList<T> implements Map<Address,Rsp<T>> {... }
----

* `RspList` is a map of `Rsp<T>` objects associated with their senders; there's one entry for each cluster node:

[source,java]
----
for(Rsp<Integer> rsp: rsps) {
    if(rsp.hasException()) {
        log.warn("exception for %s: %s", rsp.getSender(), rsp.getException());
        continue;
    }
    if(rsp.wasSuspected()) {
        System.out.printf("%s was suspected:\n", rsp.getSender());
        continue;
    }
    if(!rsp.wasReceived()) {
        System.out.printf("no rsp from %s (timeout)\n", rsp.getSender());
        continue;
    }
    System.out.printf("result for %s: %d\n", rsp.getSender(), rsp.getValue());
}
----


Rsp<T>
------
* A response is of type `Rsp<T>`:

[source,java]
----
public class Rsp<T> {
    public T         getValue() {...}
    public boolean   hasException() {...}
    public Throwable getException() {...}
    public Address   getSender() {...}
    public boolean   wasReceived() {...}
    public boolean   wasSuspected() {...}
}
----

* Method `getValue()` returns the value, or `null` if no value was received (or the method returned `void`)
* If `hasException()` is true, `getException()` will return a non-null `Throwable`
* Method `getSender()` returns the sender of this specific response
* Method `wasReceived()` returns true if the call completed successfully (even if it is a `void` method)
* Method `wasSuspected()` returns true if the member was suspected while waiting for its response



RequestOptions
--------------
* Every method invocation can be parameterized with an instance of `RequestOptions`. We can define
** Response mode: this determines whether the call is blocking and - if yes - how long it should block. The modes are:
  `GET_ALL`:: Block until responses from all members (minus the suspected ones) have been received.
  `GET_NONE`:: Wait for none. This makes the call non-blocking
  `GET_FIRST`:: Block until the first response (from anyone) has been received
  `GET_MAJORITY`:: Block until a majority of members have responded
** Timeout: max time (ms) to block. If the call hasn't completed after the timeout elapsed, a TimeoutException will be thrown.
   A timeout of 0 means to wait forever. Ignored if the call is non-blocking (mode=`GET_NONE`)
** Response filter: a `RspFilter` allows for filtering of responses and user-defined termination of
  a call. For example, if we expect responses from 10 members, but can return after having
  received 3 non-null responses, a `RspFilter` could be used. 
** Flags: the various flags to be passed to the message (see advanced section)
** Exclusion list: here we can pass a list of members (addresses) that should be excluded. For example,
  if the view is `{A,B,C,D,E}`, and we set the exclusion list to A,C then the caller will wait for
  responses from everyone except A and C. Also, every recipient that's in the exclusion list
  will discard the message.




RpcDispatcher API
-----------------

[source,java]
----
public <T> RspList<T>
       callRemoteMethods(Collection<Address> dests,
                         String method_name, Object[] args, Class[] types,
                         RequestOptions options) throws Exception;
public <T> RspList<T>
       callRemoteMethods(Collection<Address> dests, MethodCall method_call,
                         RequestOptions options) throws Exception;

public <T> T callRemoteMethod(Address dest,
                              String method_name, Object[] args, Class[] types,
                              RequestOptions options) throws Exception;
public <T> T callRemoteMethod(Address dest, MethodCall call,
                              RequestOptions options) throws Exception;
----

* The `callRemoteMethods()` (multicast) methods are invoked with a list of target
addresses. If null, the method will be invoked in all cluster nodes
** The method can be given as (1) the method name, (2) the arguments and (3) the argument types, or a
`MethodCall` (containing a `java.lang.reflect.Method` and argument) can be given instead.

* A `RspList` is returned.

* The `callRemoteMethod()` (unicast) methods take almost the same parameters, except
that there is only one destination address instead of a list.

* The `callRemoteMethod()` calls return the actual result (or type T), or throws an
exception if the method threw an exception on the target member.

* Reflection is used to find the correct method in the target node according to the method name and
number and types of supplied arguments. There is a runtime exception if a method cannot be resolved.



RpcDispatcher example
---------------------

[source,java]
----
public int print(int number) throws Exception {return number * 2;}  // <1>

RequestOptions opts=new RequestOptions(ResponseMode.GET_ALL, 5000); // <2>
JChannel channel=new JChannel();
RpcDispatcher disp=new RpcDispatcher(channel, this);   // <3>
channel.connect("RpcDispatcherTestGroup");
for(int i=0; i < 10; i++) {
    RspList rsp_list=disp.callRemoteMethods(null,      // <4>
                                            "print",
                                            new Object[]{i},
                                            new Class[]{int.class},
                                            opts);
    System.out.println("Responses: " + rsp_list);
}
----
<1> Define public method `print()`
<2> Define a `RequestOptions` object with mode=synchronous and a timeout of 5 seconds
<3> Create an `RpcDispatcher` over the channel, `this` means all methods to be invoked are in the same class
<4> Invoke the call on all cluster nodes (`null`). The method name is `"print"`, the actual argument is an array of one
element (`i`), and the formal parameters are defined with an array of class information. Finally, the RequestOptions instance
previously created is passed to the call.


Method lookup
-------------
* Use an impl of `MethodLookup` for efficient RPC marshalling
* Sends IDs (shorts) rather than method metadata across the wire
* A simple `add(3,4)` uses 125 bytes on the wire with `MethodCall(Method)` and 18 with
  `MethodCall(short)`


RpcDispatcher and `receive()`
-----------------------------
* RpcDispatcher requires `receive()` to be called, and therefore calls
  `JChannel.setReceiver()`
* If an application sets a receiver itself, this will steal all messages from
  the RpcDispatcher and it will never receive a method invocation
* Overriding a receiver with another one will log a warning


Response filters
----------------
* Response filters allow application code to drop responses, or to return from a blocking call before all responses
  have been received.
* The `RspFilter` interface looks as follows:
          
[source,java]
----
public interface RspFilter {
    boolean isAcceptable(Object response, Address sender);
    boolean needMoreResponses();
}      
----

* Method `isAcceptable()` is given a response value and the address of the member which sent
the response, and needs to decide whether the response is valid (returning true) or not
(returning false).
* The response value can be an exception if the method invocation threw an exception
* Method `needMoreResponses()` determines whether a call is done or not.



Response filter example
-----------------------
The sample code below shows how to use a RspFilter:

[source,java]
----
RspFilter filter=new RspFilter() {
    int num=0;
    public boolean isAcceptable(Object response, Address sender) {
        boolean retval=(Integer)response > 1; // <1>
        if(retval)
            num++;
        return retval;
    }
    public boolean needMoreResponses() { 
        return num < 2;                       // <2>
    }
};

RequestOptions opts=RequestOptions.SYNC().setRspFilter(filter); // <3>
RspList rsps=disp.callRemoteMethods(null, "foo", null, null, opts);
----
<1> The response filter acepts all integer values that are greater than 1. All accepted values increment a counter.
<2> The call returns as soon as (1) it has received 2 valid responses or (2) the timeout elapsed or (3) it received responses
from all members.
<3> The RequestOptions object is passed the `RspFilter` instance.



Asynchronous calls with futures
-------------------------------
* When invoking a synchronous call, the calling thread is blocked until the response (or responses) has
  been received.

* A _future_ allows a caller to return immediately and grab the result(s) later.

[source,java]
----
public NotifyingFuture<RspList>
       callRemoteMethodsWithFuture(Collection<Address> dests, MethodCall call,
                                   RequestOptions opts) throws Exception;
public <T> NotifyingFuture<T>
       callRemoteMethodWithFuture(Address dest, MethodCall call,
                                  RequestOptions opts) throws Exception;
----

* A `NotifyingFuture` extends `java.util.concurrent.Future`, with its regular methods such as `isDone()`,
  `get()` and `cancel()`. This is shown in the following code:
            
[source,java]
----
Future<Integer> future=dispatcher.callRemoteMethodWithFuture(...); // <1>
int num=future.get(); // <2>
----
<1> Here we invoke a unicast method, which completes immediately, returning a future
<2> The `get()` blocks until the result is available



Lab: RpcDispatcher
------------------
* Change ChatDemo to invoke an RPC rather than send a JGroups message for each chat message
* The callback to be invoked should be `onMessage(String message)`
** Argument `message` should contain the sender's name, or pass `sender` as an additional argument to the RPC
* Run
----
bin/run.sh org.lab.ChatDemoRpc -props config.xml -name A
----
* Extra credit: use async RPCs with futures




Distributed caching
-------------------
In this section we'll look at replicating or distributing data across a cluster. _Replication_ means that all nodes
have all the data, whereas _distribution_ means that only selected nodes store a given piece of information.

'''


ReplicatedHashMap
-----------------
* A key/value store implementing `java.util.concurrent.ConcurrentMap`
* `ReplicatedHashMap` uses _full replication_; any data added to an instance will be replicated to all other
  instances in the cluster
** Removing a key will remove it in all cluster instances
** Gets are always local and don't involve network round trips
* A new instance needs to acquire the state from an existing node (the coordinator)
* A cache can be configured to use blocking or non-blocking updates and - if blocking - define a max timeout to block
** When blocking mode is used, the caller blocks until the update has been acked by all cluster instances
** Otherwise, the call returns immediately and the updates are sent asynchronously


ReplicatedHashMap: API
----------------------

[source,java]
----
public class ReplicatedHashMap<K, V> implements ReplicatedMap<K,V> {
    public interface Notification<K, V> {   // <1>
        void entrySet(K key, V value);
        void entryRemoved(K key);
        void viewChange(View view, List<Address> joined, List<Address> left);
        void contentsSet(Map<K,V> new_entries);
        void contentsCleared();
    }
    public         ReplicatedHashMap(Channel channel) {..}       // <2>
    public void    setBlockingUpdates(boolean blocking_updates); // <3>
    public void    setTimeout(long timeout);                     // <4>
    public void    start(long state_timeout) throws Exception;   // <5>
    public void    stop();                                       // <6>
    public void    addNotifier(Notification n);                  // <7>
 // get(), put(), putIfAbsent(), remove() etc from ConcurrentHashMap // <8>
----
<1> Notification interface; can be used to register for certain events, e.g. when a key/value pair has been added
<2> Creates a new instance over an existing channel
<3> Sets the updates to be blocking or non-blocking
<4> Sets the timeout (ms); ignored if non-blocking
<5> Starts the cache: this fetches the state from an existing member
<6> Stops the cache: this stops the underlying channel and leaves the cluster
<7> Registers a listener for `Notification` callbacks
<8> The get and update methods are derived from `ConcurrentMap`; refer to it for details


ReplicatedHashMap: sample code
------------------------------

[source,java]
----
ReplicatedHashMap<String,Integer> map;
JChannel channel=new JChannel(props); // <1>
channel.connect("rhm-cluster");
map=new ReplicatedHashMap<>(channel); // <2>
map.start(); // <3>
----
<1> Creates a new `JChannel`. Note that the configuration needs to include a state transfer protocol (e.g. `STATE`), or
    else the channel creation will fail with an exception
<2> Creates a new ReplicatedHashMap over an existing (and connected) channel
<3> Fetches the state from an existing member (not done if this is the first member). If the state transfer fails,
    e.g. with a timeout, or there s no state transfer protocol configured, an exception will be thrown


ReplicatedHashMap: demo
-----------------------
* Demo: `repl-hashmap.sh`


ReplCache
---------
* A key/value store using _partial replication_: a given key/value pair is not stored on _all_ nodes, but only on
  _selected_ cluster nodes
** The number of times a key is stored is configured via a _replication count_.
** When the cluster view changes, keys are rebalanced if needed, to maintain the replication count for that key
* A put(K,V) will store K on replication count nodes
** The nodes on which K is stored are computed using a _consistent hash function_.
*** This function tries to minimize rebalancing on a view change.
* A get(K) on a node which doesn't store K is redirected to the primary owner of K
* A new instance doesn't do state transfer (like RHM above), but rebalancing might assign it some keys


ReplCache: API
--------------

[source,java]
----
public class ReplCache<K,V> {
    public interface HashFunction<K> {      // <1>
        List<Address> hash(K key, short repl_count);
    }

    public ReplCache(String props, String cluster_name);        // <2>
    public void setDefaultReplicationCount(short repl_count);   // <3>
    public void setHashFunction(HashFunction<K> hash_function); // <4>
    public void start() throws Exception;                       // <5>
    public void stop();                                         // <6>
    public void put(K k, V v, short repl_count, long timeout, boolean sync); // <7>
    public V    get(K key);                                     // <8>
    public void remove(K key, boolean synchronous);             // <9>
}
----
<1> Interface which defines the consistent hash function to be used.
<2> Creates a new instance, creating a channel first.
<3> Sets a default replication count. Used when not explicitly passed to update methods
<4> Sets the consistent hash function. There's a default if not set.
<5> Joins the cluster. This causes rebalancing if there are existing cluster members.
<6> Leaves the cluster. This causes rebalancing if there are existing cluster members.
<7> Adds data to the cache. If synchronous, `timeout` (ms) defines how long the caller is willing to block
<8> Returns a value for a given key K. If K is not local, this may result in a network round trip to the primary owner.
<9> Removes a key from all nodes which store it.


ReplCache: sample code
----------------------

[source,java]
----
ReplCache<String,String> cache=new ReplCache<>(props, cluster_name);
cache.setCallTimeout(rpc_timeout);
cache.start();
----


ReplCache: demo
---------------
* Demo: repl-cache.sh



Difference between ReplicatedHashMap and ReplCache
--------------------------------------------------
* For large clusters or large data, RHM won't scale (data-wise), as each node stores data from all other nodes
   -> use ReplCache
* When the cluster is small or the data set is small -> use RHM (all gets are local)
* RHM requires state transfer; ReplCache requires rebalancing
* Example: 5 node cluster, we need to store 10 items of 50MB each in the cluster
* RHM: each node stores 500MB, independent of cluster size
* ReplCache (repl_count=2): each node stores 2 * 10 * 50 / 5 = ~200MB on average (depends on the hashing function)



Distributed locks
-----------------

Nodes can acquire cluster-wide locks.

'''


Cluster wide locks
------------------
* Locks that can be accessed by any cluster node
* A lock is identified by a name
** All nodes accessing a lock with the same name will block on the same lock
* Locks implement `java.util.concurrent.lock.Lock`
** The same semantics are provided
*** The owner of a lock is always a thread in a node
*** Different threads in the same node will compete for locks
*** This can be turned off: lock owners can be entire nodes
**** All threads in the same node will then not block on the same lock
* Conditions are supported, too


LockService
-----------
* `LockService` provides methods to get cluster wide locks:

[source,java]
----
public LockService(JChannel ch);       // <1>
public Lock getLock(String lock_name); // <2>
----
<1> Create a `LockService` instance on top of an existing channel. The channel needs to contain `CENTRAL_LOCK`
<2> Get a lock with a given name

* Sample code to obtain a lock:

[source,java]
----
JChannel ch=new JChannel(props);
LockService lock_service=new LockService(ch);
ch.connect("lock-cluster");
Lock lock=lock_service.getLock("mylock"); // <1>
lock.lock();                              // <2>
try {
    // access a resource protected by the lock
}
finally {
    lock.unlock();                        // <3>
}
----
<1> Use the lock service to obtain a named lock
<2> Acquire the lock
<3> Release the lock


CENTRAL_LOCK
------------
* Protocol implementing the distributed locking functionality
** Needs to be at the top of the stack
* Locks are managed by the _coordinator_
* All lock requests (lock(), unlock()) are sent to the coordinator
** The coordinator keeps track of locks and their state (locked / unlocked / lock owner)
** This state can be replicated to another node (`num_backups`)
* Property `use_thread_id_for_lock_owner` determines the lock owner
** `true`: threads in the same node accessing the same lock block each other
** `false`: threads in the same node accessing the same lock don't block each other
* Config sample:

[source,xml]
----
<config>
    <UDP />
    ...
    <pbcast.GMS print_local_addr="true" join_timeout="3000"
                view_bundling="true"/>
    <MFC max_credits="2M" min_threshold="0.4"/>
    <FRAG2 frag_size="60K"  />
    <CENTRAL_LOCK num_backups="1"/>
</config>
----


LockService demo
----------------
* Demo: `lock.sh`
* E.g.: `lock.sh -props /home/myhome/lock.xml -name A`



Distributed locks and network partitions
----------------------------------------
* If we have `{A,B,C,D,E}` all locks are managed by `A`
* Let's assume we have a network split ('split brain'): `{A,B}` and `{C,D,E}`
* `A` continues managing the locks, but now `C` becomes lock coordinator for the second partition
-> The same lock can now be held by a member of `{A,B}` and by a member of `{C,D,E}` !
* Strategies to handle this:
** Define a static membership majority (e.g. 3) and become read-only (release all locks and don't acquire new ones)
   when the membership drops below 3
** Handle MergeViews: release and re-acquire all currently held locks
** Use a consensus based system, e.g. jgroups-raft: https://github.com/belaban/jgroups-raft
*** Becomes unavailable when membership drops below a given majority
*** Locks can only get acquired and released by majority agreement
*** Persistent logs allow newly elected leaders to get current lock information
*** Only _one_ member holds a lock at any time



Distributed counters
--------------------

Cluster wide atomic counters.

'''

CounterService
--------------
* Obtains named cluster wide atomic counters:

[source,java]
----
public class CounterService {
    public CounterService(JChannel ch); // <1>
    public Counter getOrCreateCounter(String name, long initial_value); // <2>
    public void deleteCounter(String name); // <3>
}

----
<1> Creates a `CounterService` instance over an existing channel
<2> Returns an existing counter, or creates a new one if none exists
<3> Deletes a counter instance (on the coordinator)

NOTE:
`CounterService` requires `COUNTER` to be somewhere near the top of the stack


Counter
-------
* Get and set a named counter to a value
* Atomic compare-and-set and incr/decr operations

[source,java]
----
public interface Counter {
    public long get();                                      // <1>
    public void set(long new_value);                        // <2>
    public boolean compareAndSet(long expect, long update); // <3>
    public long incrementAndGet();                          // <4>
    public long decrementAndGet();                          // <5>
    public long addAndGet(long delta);                      // <6>
}
----
<1> Gets the current value of the counter
<2> Sets the counter to a new value
<3> Atomically updates the counter using a CAS operation
<4> Atomically increments the counter and returns the new value
<5> Atomically decrements the counter and returns the new value
<6> Atomically adds the given value to the current value


CounterServiceDemo
------------------
* `counter.sh` / `counter.bat`
* Config:

[source,xml]
----
<config>
    <UDP />
    ...
    <pbcast.GMS print_local_addr="true" join_timeout="3000"
                view_bundling="true"/>
    <MFC max_credits="2M" min_threshold="0.4"/>
    <FRAG2 frag_size="60K"  />
    <COUNTER num_backups="1"/>
</config>
----
* Regarding merging, the same caveats as for distributed locks hold for distributed counters





Distributed task execution
--------------------------

Execution of tasks on different nodes in the cluster.

'''

Goal
----
* Oftentimes, tasks need to be executed across a distributed system, reasons are:
** We don't want to overload a single host
** If a task is long running and/or requires a lot of CPU/memory, we want to distribute the tasks across a cluster to
   harness the processing power and memory provided by individual cluster nodes
* Requirement: jobs / tasks need to be able to be broken into sizable chunks, that can be distributed in parallel
* If a node processing a task T crashes, T needs to be processed by some other node
* No single point of failure


Design
------
* Cluster of nodes, each node can submit tasks (to be executed by some other node in the cluster)
** Every node is a peer: it can both _submit_ and _handle_ tasks
** In a real application, clients could connect to _any_ node, e.g. via TCP or RMI, and submit
  tasks to that node, which would then distribute it to some other node (or handle it itself)
* When submitting a task, we pick a random integer which is then mapped to the rank of a node in the cluster
** E.g. using `modulo` or a consistent hash
* The task is then multicast (EXECUTE) across the cluster
* Every node adds the task to a hashmap consisting of tasks and their submitters' addresses
* Every node now compares the rank shipped with the task to its own rank
** It it doesn't match -> nothing is done
** Else -> the node needs to process the task. It does so and returns the result to the submitter
* When the submitter receives the response (RESULT), it multicasts a REMOVE message across the cluster
** Upon reception of REMOVE(T), every node removes T from its hashmap
* If a node X crashes (or leaves gracefully), we know which tasks were assigned to it by looking up the tasks
  in the hashmap, keyed by X
** All tasks which are still present in the hashmap have not yet been processed and need to be re-executed,
   this time by a different node
** This is done by comparing the rank shipped with the task to the node's rank and executing it if a node's
   own rank matches it
* If a node M crashes after having submitted a few tasks but not yet having received the results,
  the slaves remove all tasks submitted by M, because M won't need the results anymore.

Example
-------
* The cluster consists of nodes A, B, C and D
* Clients can access one of them
* A task submitted for example to B by a client might assign 23 to the task
* B then multicasts an EXECUTE(23, TASK) message to all nodes in the cluster, and every node adds task #23 to its cache
* However, the only node processing task #23 is A (to which 23 happens to map to),
  which then sends the result as a RESULT(23, OBJ) to B
* B returns the result OBJ to the client and multicasts a REMOVE(23) message to the cluster,
  which causes all nodes to remove task #23 from their caches.
* Had A crashed during processing of task #23, some other node would have taken over,
  processed the result and sent it back to B



Demo
----
* `task.sh` / `task.bat`
** Start a few instances
** Submit a long running job T
** Identify the host H on which T is executed
** Kill H -> T should now be executed by some other node
* Details: link:$$https://github.com/belaban/TaskDistribution$$[https://github.com/belaban/TaskDistribution]

