

Status JGroups 2014
===================
:author:    Bela Ban
:backend:   slidy
:max-width: 45em
:icons:

Agenda
------
* 3.6.0, 3.6.1
* Cluster tests on Google Compute Engine
* 4.0.0
* Workshop
* Docker
* vert.x plugin
* jgroups-raft
* etcd / Zookeeper




JGroups 3.6.0
-------------

=== UDP: ip_ttl is ignored
* `ip_ttl` is ignored (always 1)
* Regression in 3.5.0 because we now use `DatagramSocket` rather than `MulticastSocket` to send multicasts
** Needed because we need the sender's real address (not `232.5.5.5` !) to discard selected traffic
* https://issues.jboss.org/browse/JGRP-1880

=== RELAY2 doesn't work over FORK
* https://issues.jboss.org/browse/JGRP-1865

=== mcast
* Replacement for McastSender/ReceiverTest
* https://issues.jboss.org/browse/JGRP-1832

=== Docker images for JGroups
* Discussed later
* https://issues.jboss.org/browse/JGRP-1840

=== RSVP: option not to block caller
* RSVP by default blocks the caller until the ack has been received
* Message flag `RSVP_NB` (Non-Blocking) returns immediately, but keeps resending the beacon
* Further optimizations (1 beacon for all callers)
* https://issues.jboss.org/browse/JGRP-1851


=== NAKACK2 / UNICAST3: size of retransmit message is not limited
* When too many messages are retransmitted, the retransmit message itself could become too big and could get dropped
* Also, asking a sender to retransmit a lot of messages can compound a problem (e.g. GC or network bandwidth issue)
* Solution:
** Retransmit only the N oldest messages
** Use of a bitset for the missing messages
*** The size of the FixedBitSet is computed as highest-received - highest-deliverable, e.g. 550-499 == 51.
*** This would create a FixedBitSet with offset=550 and 1 long (can represent 64 seqnos).
*** The number of messages to ask for retransmission is computed as follows:
**** TP.max_bundle_size * 8, e.g. 64'000 / 8 * 64 = 512'000.
**** This means that we can store 8'000 longs in 64'000 bytes, and each long can represent 64 seqnos.
**** In this case, the max size of the FixedBitSet should therefore be 512'000.
**** This can be configured (`max_xmit_req_size`)
* https://issues.jboss.org/browse/JGRP-1868

=== UNICAST3
* Optimizations and traffic reduction for edge cases
* https://issues.jboss.org/browse/JGRP-1874
* https://issues.jboss.org/browse/JGRP-1875


=== Bind address 0.0.0.0 throws exception
* Prevents misconfiguration
* Caused mostly by running WildFly with `-b 0.0.0.0`
* https://issues.jboss.org/browse/JGRP-1885

=== CENTRAL_LOCK: make the node the lock owner, not the thread
* Lock owner is by default a thread, so threads in the same node compete for locks
* If this is enabled, then threads don't compete for the same locks
** Lock granularity is on the node level, not thread level
* Needed by the vert.x JGroups plugin
* https://issues.jboss.org/browse/JGRP-1886

=== JChannel initialization was synchronized
* Unneeded when not using a shared transport
* Slowed down Infinispan unit tests which created many channels in parallel
* https://issues.jboss.org/browse/JGRP-1887




JGroups 3.6.1
-------------

=== FD_HOST: reduction of false suspicions
* FD_HOST is used to detect entire host failures, e.g. when we have multiple nodes running on the same host
* https://issues.jboss.org/browse/JGRP-1898


=== Discovery returned after 16 responses
* Might lead to split cluster groups on startup and unneeded merging later
* https://issues.jboss.org/browse/JGRP-1899


=== TCPPING: static discovery does not have to list all members anymore
* Frequently requested feature
* The coordinator can now send member information to all other members
* Can be enabled in configuration (`send_cache_on_join`)
* Can also be triggered via JMX (`sendCacheInformation()`)
* https://issues.jboss.org/browse/JGRP-1903


=== NAKACK2: retransmit the last missing message sooner
* We don't want to wait for STABLE to kick in
** STABLE requires consensus from all members to reach stability
* Can be set with `resend_last_seqno`
* If enabled, multicasts the highest sent seqno every `xmit_interval` ms
* Shuts up after `resend_last_seqno_max_times` times when no new messages have been sent
* https://issues.jboss.org/browse/JGRP-1904



Cluster tests on Google Compute Engine
--------------------------------------
* Large cluster testing due to a generous donation by Google
* Introduction: http://belaban.blogspot.ch/2014/04/running-jgroups-on-google-compute-engine.html
* 1000 JGroups cluster: http://belaban.blogspot.ch/2014/07/running-jgroups-cluster-in-google.html
** Perf test: UPerf
** Roughly *3'800 reqs/sec/node* (20'000 reqs/node, 20% writes, 80% reads, 25 sender threads/node, 1k requests)
** Record JGroups cluster size: 2286 nodes
* 500 node JDG cluster: http://belaban.blogspot.ch/2014/07/new-record-for-large-jdg-cluster-500.html
** 2'300 reqs/sec/node
* Results:
** `GOOG_PING` was written
** Numbers for perf tests on large clusters
** Discovery protocol was improved (static info for cloud based cluster discovery)



JGroups 4.0
-----------
* API changes
** Trashing UNICAST, UNICAST2, NAKACK
* NIO.2
** Direct buffers
** Reducing of buffer copying
** Scattering and gathering
** Selector model for UDP and TCP ?
* See status slides for 2014
* Postponsed until 2015
** Large disruptive change for JGroups and Infinispan
** Workshop and RAFT have higher priority


JGroups workshop
----------------
* Internal and public workshop on JGroups
* Modules for
** API and use
** Building blocks (distributed locks, counters etc)
** Advanced
** Protocols
** Admin
** Labs (optional)
* A la carte mix-and-match, e.g.
** 3 day workshop with API, Building blocks and Admin, plus 2 days of consulting, on-site
* Availability: spring 2015
** To be held 2x in Europe and 2x in the US in 2015
* https://github.com/belaban/workshop


Docker
------
* Docker image containing JGroups and a few scripts to start demos: https://registry.hub.docker.com/u/belaban/jgroups/
* Blog: http://belaban.blogspot.ch/2014/10/jgroups-and-docker.html
* GitHub: https://github.com/belaban/jgroups-docker
* To run: `docker run -it belaban/jgroups`:
----
Welcome to JGroups !
********************

Please read the README.md file for detailed instructions on how to run
the demos.

The following demos can be run (run them in multiple containers):

* chat [-name name]

* lock [-name name]

* count [-name name]


Questions can be asked on the users or dev mailing lists:
https://sourceforge.net/p/javagroups/mailman.

Enjoy !

Bela Ban


[jgroups@64e1c5a3afa1 ~]$
----


vert.x clustering plugins
-------------------------
* Fabio wrote 2 clustering plugins for vert.x: Infinispan based and JGroups based
* Goal: to replace Hazelcast, which is shipped by default by vert.x
* Presentation Fabio


RAFT consensus in JGroups: jgroups-raft
---------------------------------------

=== Why
* Experiment with consensus in the face of network partitions
* Possibly use in Infinispan: **C**A**P**
* JGroups-internal services might benefit, too: distributed counter and locks, sequencer (?)
** Sacificing availability for consistency
* At the other side of C**AP** (eventual consistency)
** Sacrificing consistency _temporarily_ for availability
* Implementation of etcd / ZooKeeper interfaces

=== What
* RAFT is a consensus based algorithm for replicated state machines
** Consensus used for leader election and committing of changes
* Fixed size cluster (e.g. of 5): `{A,B,C,D,E}`
* _Leaders_ and _followers_; leaders are elected by majority votes (3) (and highest commit log)
** There's always only *a single leader* in a cluster
** All requests are handled by the leader
** Minority partitions (e.g. 2 or less) become unavailable; client requests are rejected
* Leader `A` receives a client request and appends it to its persistent log
* `A` then sends a message to followers `B`, `C`, `D` and `E` which append the change to their logs
* When A receives an ack from 2 other members (majority == 3), it _commits_ the change and applies it to its
  attached _state machine_
* `A` periodically sends its highest commit ID (also acting as a heartbeat) to all followers, who then commit as well
* When the followers update their commit index, they also apply the changes to their state machines
* Similar to `SEQUENCER` (total order), except that `SEQUENCER` can have multiple coordinators in case of network partitions


=== Difference to 2PC
* In 2PC, _all_ TX participants have to ack the prepare
* In RAFT, only a majority needs to ack
** Slow members don't slow everybody else down
* Members which are completely out of sync, or new joiners without a log get sync'ed via snapshot installation
** Similar to state transfer


=== How
* Implementation of the RAFT consensus algorithm [1] in JGroups
* Separate project *jgroups_raft* [2], might get merged into JGroups at some point
* New protocols `ELECTION`, `RAFT` and `CLIENT` (client redirection to the current leader)
* `StateMachine` interface, demo implementation `ReplicatedStateMachine` and demo `ReplicatedStateMachineDemo`
* Persistent `Log` interface with impls based on MapDB (JDBM4) and LevelDB (default) (Ugo)
* Demo

=== Status
* Version 0.1 (alpha quality)
* Includes log appending, log compaction and snapshotting
* No dynamic view installation yet
** Static cluster with static majority
* Can be used to experiment with RAFT consensus
* OK for etcd development


=== Todo list
* Use JGroups' failure detection and view installation to replace part of the current code (less code)
* Full unit test coverage
* Dynamic view installation



=== References
* [1] http://raftconsensus.github.io/
* [2] https://github.com/belaban/jgroups-raft
* [3] https://github.com/belaban/jgroups-raft/tree/master/doc


Implementation of etcd over jgroups-raft
----------------------------------------
* Fabio, Ugo
* Presentation Fabio/Ugo
* Possibly provide Zookeeper API ?