
Handling network partitions
===========================
:author: Bela Ban belaban@yahoo.com
:backend: deckjs
:deckjs_transition: fade
:navigation:
:deckjs_theme: web-2.0
:deckjs_transition: fade
:goto:
:menu:
:toc:
:status:






Network partitions (split brain)
--------------------------------
* Cluster: `v4={A,B,C,D,E}` (coord=`A`)
** Members `{D,E}` are in a different subnet, connected to `{A,B,C}` by a switch
* Assume the switch connecting the 2 subnets fails -> the network is partitioned into `{A,B,C}` and `{D,E}`
** The members in `{A,B,C}` can ping each other, but not `{D,E}`, and vice versa
** Each partition thinks the other one is dead -> *split brain*
* JGroups detects this and creates 2 subclusters: `{A,B,C}` (`A` remains coordinator) and `{D,E}` (`D` becomes coord)
** These views may not be installed at exactly the same (wall clock) time
*** E.g. `A` may have installed view `{A,B,C}`, but `E` may still have view `{A,B,C,D,E}` before `{D,E}` is installed
* Clients may be able to access one or both of the partitions (or none)
* When the partition heals, JGroups will merge the subclusters back into _MergeView_ `v6={A,B,C,D,E}`
** A MergeView has a list of all subviews (partitions)
*** We know which members were part of which partition before the split healed
* However, *JGroups won't be able to merge the application data*


Data inconsistency
------------------
* Partitions `{A,B,C}` and `{D,E}`
* Clients might be able to access both, one or none of the partitions
* A client sets `x=10` in `{A,B,C}` and a different client sets `x=20` in `{D,E}`
* What do we do when the partition heals?
** MergeView: `{A,B,C,D,E}`

image::../images/NetworkPartition.png[Data inconsistency,width="80%",align=left,valign=top]



Strategies for keeping application data consistent
--------------------------------------------------
* Basically 2 strategies:
. Merge the divergent data
** Decide whether `x` should be `10` or `20` in all members of the merged view
** How?
*** Timestamps? Counters?
. Prevent divergent data altogether
** Prevent clients from writing to either partition, or
** Clients can only write to the majority partition `{A,B,C}`


CAP
---
* **C**onsistency, **A**vailability, **P**artition handling
* CAP: either CP or AP (P can never be forfeited as partitions do happen)
* AP: availability & partition handling
** The system may not always be consistent
** Eventual consistency: possibility to see stale data but eventual convergence of data
** Example: Amazon's Dynamo
* CP: consistency & partition handling
** The system is always consistent
** The system may not be available all the time
** Example: Raft (jgroups-raft)



Availability and partition handling (AP)
----------------------------------------
* All partitions are allowed to make progress (read-write)
* Partitions can diverge if the same data is modified in different partitions
* When the network partition heals, data has to be merged
* Merge strategies:
** Timestamps, physical time, logical clocks
** Member precedence
** Causal vectors (*eventual consistency*)
*** Has to contact application if data collision cannot be resolved automatically
* Advantage: system is always available and accepts writes
* Disadvantage: merging data can be hard (and we may have to consult the application)



Eventual consistency (EV)
-------------------------
* "Dynamo: Amazon's Highly Available Key-Value Store"
** http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf
* Each data has a version and a vector clock (array of members plus update count)
* A write updates the vector at the member which performed the write, e.g.:
** `x=10 (v26) A[3], B[2], C[1]` // 3 updates from A, 1 from C and 2 from B
** `x=20 (v27) A[3], B[3], C[1]` // successor to `v26`, B made one more update
* If a vector clock `V2` has the same fields as `V1` and all values are smaller or equal, then `V2` is causally
  related to `V1` (ancestor) and `V1` can be dropped. `v26` above is an ancestor to `v27`
* Otherwise, two vectors are parallel and a read needs to reconcile them:
** `x=10 (v14) A[2], B[3], C[1]`
** `x=20 (v17) A[2], B[2], C[2]`
*** Here, neither vector is a descendent of the other, and therefore, the client needs to be asked for the correct
    value (`x=10` or `x=20`).


Properties of eventual consistency
----------------------------------
* Available even during network partitions (AP)
* Reads and writes always succeed
* Reads may have to ask the client to reconcile divergent data (callback or policy to handle collisions)
* Reading stale data is possible




Consistency and partition handling (CP)
---------------------------------------
* Only a majority partition is allowed to perform reads and writes
** The majority partition can also be defined differently as long as the decision is deterministic
   (only one majority partition), e.g.
*** A given node needs to be present
*** Access to a given resource (e.g. DB)
**** Whoever has access to the DB is allowed to write, others shut down or become read-only
* A minority partition rejects client access (stale reads might be allowed)
* Advantage: no merging of data
* Disadvantage: system unavailable when no majority


Example: primary partition handling
-----------------------------------

[source,java]
-----
static final int majority=3;
boolean is_primary;

public void viewAccepted(View new_view) {
    int size=new_view.size();
    if(is_primary) {
        if(size < majority) {
            is_primary=false;
            // go into read-only mode (or reject all requests)
        }
    }
    else {
        if(size >= majority) {
            is_primary=true;
            // 1. go into read-write mode
            // 2. overwrite state with state from primary partition
        }
    }
}
-----
* A cluster becomes a primary partition as soon as it has `majority` members
* A read-only, non-primary partition exists when the view size drops below `majority` members



What's wrong with this example?
-------------------------------
* Views are not installed synchronously in all members, example:
** There's a partition between `{A,B}` and `{C,D,E}`
** At `T500`, failure detection in `{C,D,E}` detects connectivity problems, excludes `A` and `B` and installs view
  `{C,D,E}`
*** `{C,D,E}` continues to be the primary partition and accepts write requests
** At `T800`, view `{A,B}` is installed in `A` and `B`; its members are not in the primary partition anymore,
   and write requests are therefore rejected
** However, for 300ms, `A` and `B` continued to accept write requests, and both partitions were primary partitions!










