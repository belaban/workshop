
Protocols in JGroups
====================
:author: Bela Ban belaban@yahoo.com
:backend: deckjs
:deckjs_transition: fade
:navigation:
:deckjs_theme: web-2.0
:deckjs_transition: fade
:goto:
:menu:
:toc:
:status:



udp.xml
-------

[source,xml]
----
<config xmlns="urn:org:jgroups"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="urn:org:jgroups http://www.jgroups.org/schema/jgroups.xsd">
    <UDP mcast_port="${jgroups.udp.mcast_port:45588}"
         ip_ttl="4"
         ucast_recv_buf_size="5M"
         ucast_send_buf_size="5M"
         mcast_recv_buf_size="5M"
         mcast_send_buf_size="5M"
         max_bundle_size="64K"

         thread_pool.min_threads="2"
         thread_pool.max_threads="8"
         thread_pool.keep_alive_time="5000"
         thread_pool.queue_max_size="10000"
         thread_pool.rejection_policy="discard"

         oob_thread_pool.min_threads="1"
         oob_thread_pool.max_threads="8"
         oob_thread_pool.keep_alive_time="5000"
         oob_thread_pool.rejection_policy="discard"/>

    <PING />
    <MERGE3 max_interval="30000" min_interval="10000"/>
    <FD_SOCK/>
    <FD_ALL/>
    <VERIFY_SUSPECT timeout="1500"  />
    <BARRIER />
    <pbcast.NAKACK2 xmit_interval="500"
                    max_msg_batch_size="500"
                    discard_delivered_msgs="true"/>
    <UNICAST3 xmit_interval="500" max_msg_batch_size="500"/>
    <pbcast.STABLE desired_avg_gossip="50000" max_bytes="4M"/>
    <pbcast.GMS print_local_addr="true" join_timeout="2000"
                view_bundling="true"/>
    <UFC max_credits="2M" min_threshold="0.4"/>
    <MFC max_credits="2M" min_threshold="0.4"/>
    <FRAG2 frag_size="60K"  />
    <pbcast.STATE_TRANSFER />
</config>
----


Transport protocols
-------------------
* Bottom-most protocol in a stack
* Interacts with the network
* Main tasks
. Serialization and sending of messages to other nodes
. Deserialization of network buffers into messages and passing those up the stack
* Most of the resources of a node (sockets, thread pools) are located in the transport protocol
* Available transports:

[width="100%",cols="4,15", frame="topbot",options="header"]
|====
| Name | Implementation
| UDP | Sends datagram packets (to single destination) or multicast packets (to all cluster nodes). Requires IP multicasting to be enabled.
| TCP | Uses TCP sockets. A message sent to the entire cluster (N nodes) is sent N-1 times. 1 thread per connection.
| TCP_NIO | Same as TCP, but uses NIO.2: thread pool to handle _all_ connections.
| TUNNEL | Talks to external process (GossipRouter) to send/receive messages. Can be used to connect nodes behind firewalls.
| SHARED_LOOPBACK| Dummy transport to connect members running inside the same process. Used for unit testing
|====
* Ref: http://www.jgroups.org/manual/index.html#Transport



Discovery
---------
* Discovers the initial membership
* Used to find out who the _existing coordinator_ is (to send a `JOIN` request to it)
* Implementations:

[width="100%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| PING | Sends discovery request as multicast (requiring `UDP` as transport)
| TCPPING | Defines a static list of nodes (IP addresses and ports) to contact, e.g. `initial_hosts=A[7800],B[7800],C[7800]`
| MPING | Uses its own multicast socket to multicast discovery requests. Used when the transport
          doesn't support multicasting, e.g. `TCP`
| TCPGOSSIP | Contacts one or more external processes (`GossipRouter`) to find the initial membership for a given
              cluster. Each node registers with the GossipRouter(s) on startup.
| JDBC_PING | Uses a database to store the membership for a given cluster. Each members adds its own information at startup.
| FILE_PING | Uses the file system to store membership information. On startup, each members writes its own information
              to the (shared) file system
| S3_PING, GOOGLE_PING| Uses a cloud store for membership information. More impls include AWS_PING, SWIFT_PING,
                        RACKSPACE_PING, Kubernetes, Zookeeper etc
|SHARED_LOOPBACK_PING| Requires `SHARED_LOOPBACK` as transport, delegates all discovery requests to the transport
|====

* Ref: http://www.jgroups.org/manual/index.html#DiscoveryProtocols



Merging
--------
* When failure detection splits a cluster into multiple subclusters, e.g. `A15={A,B,C}` and `D15={D,E}`, merging tries to
  merge the subclusters back into a single cluster:
** The _subcluster coordinators_ `A` and `D` periodically send out merge messages
*** `A` sends `{coord=A,addr=192.166.1.5:50500,view=A15,cluster="demo"}`
*** `D` sends `{coord=D,addr=192.168.1.6:60500,view=D15,cluster="demo"}`
** When `A` or `D` encounter a message from another node claiming to be coordinator for the same cluster, they
   engage in a protocol to
*** Exchange information about their respective subclusters
*** Agree on a _merge leader_ who
**** Generates a `MergeView` (`A16={A,B,C,D,E}`)
**** Has all subcluster coordinators install the MergeView
**** All subcluster coordinators except one (e.g. `A`) step down
* The merging itself is done by `GMS`; the merge protocol only notifies `GMS` that a merge is needed

[width="100%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| MERGE3 | Uses the algorithm described avove
|====

* Ref: http://www.jgroups.org/manual/index.html#_merging_after_a_network_partition




Failure detection
-----------------
* Failure detection checks for crashed or unresponsive members and _suspects them_
** It _doesn't remove suspected members_ (that's the task of `GMS`)
* Note that members which leave _gracefully_ (e.g. via `JChannel.disconnect()`) are never handled by failure detection
* Implementations:

[width="100%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| FD_ALL | Every node periodically multicasts heartbeats. Members maintains a table of members and timestamps and
           reset the timestamp for P on reception of a heartbeat or message from P. +
           If a member's timestamp exceeds a threshold, a suspect(P) message is sent to the coordinator, which tells
           `GMS` to handle the suspicion. +
           `GMS` then removes P and installs a new view excluding P. `FD_ALL` works best with `UDP` as transport as
           heartbeats are multicast.
| FD_SOCK | Members of a cluster form a logical ring, with each member creating a TCP connection to the member to its
            right. E.g. in `{A,B,C}`, `A` -> `B` -> `C` -> `A`. +
            When `C` crashes, `B` will get notified that the TCP connection to `C` was closed and sends a
            `suspect(C)` to the coordinator (`A`), who excludes `C` from the view.
| FD | Same as `FD_SOCK` but instead of a TCP connection, heartbeat messages are sent to the neighbor to the right. If
       a timeout elapses without getting a response or traffic from the neighbor, the neighbor is suspected.
| FD_HOST| To detect the crash or freeze of entire hosts and the cluster members running on them. Not
           meant to be used in isolation, as it doesnâ€™t detect crashed members on the local host, but in conjunction
           with other failure detection protocols, such as `FD_ALL` or `FD_SOCK`. +
           Can be used when multiple cluster members are running on a box. For example, if we have members
           `{A,B,C,D}` running on host 1 and `{M,N,O,P}` running on host 2, and host 1 is powered down, then `A`, `B`,
           `C` and `D` are suspected and removed from the cluster _together_, typically in one view change.
|====

* Ref: http://www.jgroups.org/manual/index.html#FailureDetection




Reliable transmission and ordering
----------------------------------
* Sender S assigns monotonically increasing sequence numbers (seqnos) to messages
* Receiver R receives messages from S and delivers them (up the stack) in seqno order
* Negative acks: the receiver asks a sender to retransmit a message if a gap has been detected, e.g. `S5` -> `S6`
  -> `S8`: here, message `7` from `S` is missing so `R` asks `S` to resend message `7`.
* Positive acks: the sender continues to periodically retransmit a message `M` until the receiver `R` has acked
  `M`, or `R` left the cluster

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| NAKACK2 | For sending messages reliably and in (sender) order to all cluster nodes. Uses negative acks.
| UNICAST3 | For sending unicast (point-to-point) ordered messages. Uses a mixture of negative and positive acking.
|====
* Ref: http://www.jgroups.org/manual/index.html#ReliableMessageTransmission



Message stability
-----------------
* Only applicable to one-to-many (= multicast) messages (`NAKACK2`)
* When multicasting messages with `NAKACK2`, receivers store all messages in order to deliver them in order, and
  to possibly resend missing messages
* However, keeping messages around indefinitely increases the heap used and eventually members run out of memory
* `STABLE` is an agreement protocol which runs periodically (or based on received bytes) and removes messages which
  have been seen by everyone
* E.g if we have members `{A,B,C}` and receive
** From `A`: `{A:15,B:25,C:3}`
** From `B`: `{A:14,B:29,C:5}`
** From `C`: `{A:15,B:27,C:5}`
** Then the minimum vector is `{A:14,B:25,C:3}`.
*** All members can now remove messages `14` and lower from `A`, `25` and lower from `B` and `3` and lower from `C`
* Slow or unresponsive members can prevent fast agreement, but they'll eventually get removed by failure detection

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| STABLE | Implements the above algorithm
|====
* Ref: http://www.jgroups.org/manual/index.html#STABLE



Group membership
----------------
* Handles new member joining, existing members leaving or crashing (suspicions) and merging
* Adds new members to the view, removes left or suspected members from the view and installs a new view
* Also handles merge events sent by a merge protocol

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| GMS | Implements the functionality discussed above
|====
* Ref: http://www.jgroups.org/manual/index.html#GMS


Flow control
------------
* Flow control makes sure that receivers that process messages slower than senders are sending them, don't get
  overwhelmed by the traffic -> OOME
* This is done by throttling the senders: their rate is adjusted to the fastest rate at which the receiver(s) can process messages
* Senders and receivers start out with _credits_ which are bytes to send
* A sender subtracts the bytes it sends and blocks if its credits are used up
* A receiver also subtracts the credits for each message it receives from a sender and sends new credits to the sender(s)
  when the credits fall below a given threshold (e.g. 10%)

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| UFC | Unicast flow control. Maintains flow control between 2 peers for unicast messages. Can be removed if `TCP` is
        used as transport
| MFC | Multicast flow control. Maintains a credit table for _all_ receivers and senders and blocks for credits
        or sends credits based on the _slowest_ member. +
        A slow member can slow everyone down -> if it's too slow it can be suspected and removed by failure detection)
|====
* Ref: http://www.jgroups.org/manual/index.html#FlowControl



Fragmentation
-------------
* Fragments packets larger than a threshold at the sender and unfragments at the receiver
* Handles both unicast and multicast messages

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| FRAG2 | Framentation / unfragmentation with minimal copying
|====
* Ref: http://www.jgroups.org/manual/index.html#_fragmentation


Synchronous sending
-------------------
* Handles messages with an `RSVP` flag
* Blocks the sender until acks from all receivers have been received
* Can be used for unicast and multicast messages

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| RSVP | See discussion on RSVP in the advanced section
|====
* Ref: http://www.jgroups.org/manual/index.html#RSVP


State transfer
--------------
* Used for state transfer between a new joiner and the coordinator
* See section on state transfer

[width="90%",cols="3,10", frame="topbot",options="header"]
|====
| Name | Implementation
| STATE_TRANSFER | Uses byte buffers for state transfer
| STATE | Chunks state into multiple segments and sends them (flow controlled)
| STATE_SOCK | Establishes a TCP connection between state requester and provider to exchange state
| BARRIER | Blocks threads from modifying state on the state provider until state has been written to the pipe
|====
* Ref: http://www.jgroups.org/manual/index.html#StateTransferProtocolDetails


Total order
-----------
* Establishes total order for _multicast_ messages
* All multicast messages are sent to a sequencer (coordinator)
* The sequencer then sends the messages on behalf of the sender -> total order

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| SEQUENCER | See `sequencer.xml` for where to place this protocol
|====
* Ref: http://www.jgroups.org/manual/index.html#_ordering


Encryption and authentication
-----------------------------
* Encryption of unicast and multicast messages
** Done using a shared key
*** Via a keystore or asymmetric key exchange (redone on each view change)
* Authentication
** Admission of new members to a cluster only if they are authenticated

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Implementation
| ENCRYPT | Encryption on the sender and decryption on the receiver(s)
| AUTH | Admission of new cluster members only when authentication succeeds.
| SASL | SASL based authentication
|====
* Ref: http://www.jgroups.org/manual/index.html#Security



Miscellaneous
-------------

[width="90%",cols="3,10", frame="topbot",options="header"]
|====
| Name | Implementation
| RELAY2 | Relaying of messages between separate clusters. See advanced section
| STOMP | Client access via Stomp to _any_ cluster node. Stomp messages received by a client are sent to the entire cluster,
          where all connected clients receive them. +
          Clients can be written in any language and only need to implement the Stomp protocol.
| COMPRESS | Compression of the payload of messages on the sender and decompression on the receiver(s)
| FORK | Light-weight channels. See discussion in advanced section.
| CENTRAL_LOCK | Distributed locking. See discussion in advanced section.
| COUNTER | Distributed atomic counters. See discussion in advanced section.
|====
* Ref: http://www.jgroups.org/manual/index.html#Misc








