
Admin
=====
:author: Bela Ban belaban@yahoo.com
:backend: deckjs
:deckjs_transition: fade
:navigation:
:deckjs_theme: web-2.0
:goto:
:menu:
:toc:
:status:



Tuning
------
* Let's take a (non exhaustive) look at things that can be tuned
* Ref: https://developer.jboss.org/wiki/PerfTuning



Multicast routing
-----------------
* When using `UDP`, IP multicasting is required
* On some systems, multicast route(s) need to be added to the routing table
** Otherwise, the default route will be used
*** Note that some systems don't consult the routing table for IP multicast routing, only for unicast routing
* MacOS example:
----
# Adds a multicast route for 224.0.0.1-231.255.255.254
sudo route add -net 224.0.0.0/5 127.0.0.1

# Adds a multicast route for 232.0.0.1-239.255.255.254
sudo route add -net 232.0.0.0/5 192.168.1.3
----
** When binding to `127.0.0.1`, `UDP.mcast_addr` should be set to a value between `224.0.0.1` and `231.255.255.254`
*** `224.0.0.1` - `224.0.0.255` not recommended, can get dropped by switches
** When binding to `192.168.1.3`, `mcast_addr` should be set to a value between `232.0.0.1` and `239.255.255.254`
* Alternative: use <UDP `receive_on_all_interfaces="true" ... />`



Switching from UDP to TCP
-------------------------
* Copy `udp.xml` -> `tcp.xml`
* Change `UDP` to `TCP`
* Remove `mcast_send_buf_size`, `mcast_recv_buf_size`, `ip_ttl`, `tos`, `mcast_port`
* Change `ucast_send_buf_size` -> `send_buf_size`
* Change `ucast_recv_buf_size` -> `recv_buf_size`
* Add `bind_port="7800"` (for example) to `TCP`
* Rename `PING` to `TCPPING`, remove all attributes
** Add `initial_hosts=A[7800],B[7800],C[7800]` to `TCPPING`
*** `A` is the `bind_addr` of host A, `B` of host B etc
*** I'm assuming here that `bind_port` is `7800` everywhere
* Run a few Draw instances with -props ./tcp.xml
* Verify that `bind_addr`, `bind_port` and `initial_hosts` are OK:
----
[mac] /Users/bela/workshop/bin$ ./probe.sh jmx=TCP.bind jmx=TCPPING.init

#1 (344 bytes):
TCP={bind_addr=/192.168.1.3, bind_port=7800}
cluster=draw
view=[A|1] (2) [A, B]
physical_addr=192.168.1.3:7800
local_addr=A [1253cca8-19e8-068a-b773-5e2d3771f507]
TCPPING={initial_hosts=[192.168.1.3:7800, 192.168.1.3:7801, 192.168.1.3:7802]}


#2 (344 bytes):
TCP={bind_addr=/192.168.1.3, bind_port=7800}
cluster=draw
view=[A|1] (2) [A, B]
physical_addr=192.168.1.3:7801
local_addr=B [d77986dd-681c-a7fd-f6b7-bd2800c8b2f9]
TCPPING={initial_hosts=[192.168.1.3:7800, 192.168.1.3:7801, 192.168.1.3:7802]}

[mac] /Users/bela/workshop/bin$
----




Buffers
-------
* _Right size_ buffers
** Too small: packet drops, too big: wasted resources
** Bandwidth-delay product only works for point-to-point (TCP)
*** In the worst case, we need to multiply this by the number of senders (oversimplification)
**** Max traffic we can receive is `min(current_senders * rate, link_bandwidth)`
* Transport buffers:
** `UDP`: `mcast_send_buf_size`, `mcast_recv_buf_size`, `ucast_send_buf_size`, `ucast_recv_buf_size`
*** Linux: these buffer cannot be bigger than `net.core.rmem_max` (recv) or `net.core.wmem_max` (send)
** `TCP`: `send_buf_size`, `recv_buf_size`
* NIC input buffers: `/sbin/ifconfig txqueuelen 5000` (Linux)


////
Flow control
------------
* Multicast flow control: `MFC`, unicast flow control: `UFC`
* `UFC` not needed when the transport is `TCP`
* The more credits (`max_credits`) a sender has, the more data it can send until it blocks
* A higher `min_threshold` value leads to quicker credit replenishments by the receivers back to the senders
* However: if `max_credits` and `min_threshold` are too large, then the purpose of flow control is defeated
** Receivers might still run out of memory as they're getting more messages than they can handle
* Suggestion: test with load that's slightly higher than expected load, watch memory use over time
////

////
Transport resources
-------------------
* Thread pool buffers (regular, OOB, incoming, timer), queues
* All 4 thread pool use j.u.c.ThreadPoolExecutor with its semantics
** Create min threads, then fill queue (if enabled), then create up to max threads, then reject
* Hands off the internal pool (used by JGroups only) !
* Timer pool should not be changed either, unless we expect a lot of timer tasks, or long running tasks
* This leaves us with the default and OOB pools
////

Transport: default thread pool
------------------------------
* For regular (sender-FIFO) messages
* (Conceptual) queues are created for each sender
* Only 1 thread processes a queue, delivering _1 message at a time_
* The other messages for the same sender consume threads only to add the messages to the queue, then the thread is
  put back into the pool
* Recommendations for peak (receiving messages from N senders concurrently):
** Set min-threads to N
** Set max-threads to N+2 (2 spare threads)
** Enable a queue to catch traffic peaks


Transport: OOB thread pool
--------------------------
* For OOB messages (no defined delivery order)
* Each thread takes a message (or message batch) and passes it up to the application
* Many messages from the same sender can be processed concurrently
* Recommendations:
** Disable the thread pool queue
** Set min-threads to a small number (more threads will be created if needed)
** Set max-threads to the max number of OOB messages expected to be received concurrently
*** This number can be high because we won't reach it unless we have many concurrent messages
**** The thread idle time will reduce the active thread size after a while if not all threads > min-threads are used
* Thread stack size: uses memory, make sure to size this as well


////
Ethernet flow control 802.3x
----------------------------
* Good for `UDP`, bad for `TCP`
* Enable: `/sbin/ethtool -A eth0 tx on rx on` (Linux)
* Enable in switch as well
* Ref: https://developer.jboss.org/wiki/PerfTuning


Interrupt coalescing
--------------------
* Collects multiple interrupts and handles them together
* Less 'context switching'
* Slightly worse latency
* Example: `/usr/sbin/ethtool -C eth0 rx-usecs 75`
////


Jumbo frames
------------
* Increases the size of a datagram packet's MTU, e.g. from 1500 to 8000
* If we send large messages, fewer datagrams need to be sent
** 60'000 byte message: 40 packets with mtu=1500, 8 with mtu=8000
** `UDP`: if 1 datagram packet of a message is lost, we need to retransmit all IP packets
*** Smaller chance of dropping 1 out of 8 packets than 1 out of 40
* Excellent for high throughput
* Needs to be enabled on all hosts and the switch(es)


UDP versus TCP
--------------
* `UDP` sends 1 multicast packet to the switch, which copies it to all ports with subscribers for the multicast group
** Cost to send a group message to all cluster members: 1
* `TCP` sends the message to each member separately
** Cost: N-1 (where N is the cluster size)
** If `N-1 * message size` is larger than the link's bandwidth, this is a bottleneck
* TCP generates more traffic for group messages
* UDP more scalable in large clusters



Message bundling / batching
---------------------------
* JGroups by default queues smaller messages on the sender until a size threshold has been exceeded, or no more
  messages are available
----
loop
    while(queue not full and more msgs available)
        queue next message
    send message batch
endloop
----
* Sends a single message immediately (low latency)
* Sends many messages in the time it takes to add them to the queue and exceed the size threshold
* Queued messages are then sent as one big message
* Advantage: payload-to-header ratio is better, less overhead per message
* Batching can be bypassed by marking a message as `DONT_BUNDLE` and `OOB`
** Only recommended for selected (few) messages


Retransmission intervals in NAKACK2 and UNICAST3
------------------------------------------------
* Attribute `xmit_interval` defines the interval at which we're checking for missing messages and ask the sender
  for retransmission (NAKACK2,UNICAST3), or resend messages for which we haven't yet received an ack (UNICAST3)
* A small interval might lead to multiple redundant retransmission requests/responses
** This increases traffic and might compound the problem -> even more dropped packets due to buffer overflow
* If the interval is too high, retransmission may not be able to retransmit all missing messages (see next topic) in one go


////
Maximum size of retransmission requests
---------------------------------------
* In `NAKACK2` and `UNICAST3`, if too many messages are missing, a retransmit request message may become too big
* Only applicable to `UDP`
* Both protocols therefore only request for retransmission of the oldest N messages, such that the size of the retransmit
  request doesn't exceed the max datagram packet size
* The max size of a retransmit request can be configured: `max_xmit_req_size`
////


STABLE
------
* Purges messages seen by everyone in `NAKACK2`
* Low stable interval -> quick purging but more traffic
* High stable interval -> less traffic but memory accumulation
* Find the optimal tradeoff based on traffic pattern
* STABLE rounds can also be triggered manually / programmatically (`STABLE.gc()`)



Link bundling
--------------
* Logical network interface, but consisting of multiple physical NICs
* Each physical NIC might use a different network -> multiplies bandwidth
* Example: IP bonding (Linux)




Top JGroups problems
--------------------
* From
** Mailing lists
** Support cases
** Consulting
** Interaction with customers
** Bug reports





Problem #13: AWS
----------------
* Large packets sizes in EC2 are dropped
** The problem was that large packets using the default stack configuration for `FRAG2` (60k) were sometimes being dropped
   between some hosts.
** The cluster would work fine until a large amount of data was sent between some pairs of servers.
** Amazon support: this is an update for case 85983221. We are currently limited to packet sizes of 32k and below on Amazon
   EC2 and can confirm the issues you are facing for larger packet sizes. We are investigating a solution
   to this limitation. Please let us know if you can keep your packet sizes below this level, or if this
   is severe problem blocking your ability to operate.
* Solution: use `FRAG2` sizes of <= 32k if you are running in `UDP` mode under EC2.


Problem #12: last message dropped in `NAKACK2`
---------------------------------------------
* Last message dropped issue
* Solution: use `RSVP` to ack a batch of work, or set `resend_last_seqno` in `NAKACK2`




Problem #11: `receive()` and `viewAccepted()` callbacks
------------------------------------------------------
* Invoking blocking RPCs or doing something long or blocking in these callbacks
* Because JGroups calls these callbacks on a thread from the incoming thread pool, all messages behind this one are stuck
  until the callback returns
* Solution: use a separate thread is some callback code needs to block, invoke a blocking RPC, or perform a long task


Problem #10: reusing a message (the Sebastian problem)
------------------------------------------------------
[source,java]
----
Message msg=new Message(null, "hello");
for(int i=0; i < 10; i++)
   channel.send(msg)
----
* Spot the problem ?


Problem #9: MacOS
-----------------
* Multicast routing on Mac OS: https://developer.jboss.org/wiki/MulticastRoutingOnMacOSX
* Solution: pick the correct `mcast_addr` in `UDP` based on the routing table and `bind_addr`



Problem #8: JGroups eating memory: NAKACK2 / STABLE
---------------------------------------------------
* Memory grows in `NAKACK`
* In most cases, this is caused by a slow member which hasn't yet been suspected and excluded (hinders progress)
* Symptom: one or more slow members prevent an agreement between all members on which messages have been seen and can
  be discarded -> memory accumulates
* Solution: remove / fix the slow or unresponsive members or decrease the failure detection timeout to exclude the member




Problem #7: seeing traffic from different clusters
--------------------------------------------------
* When using UDP, we get warnings that traffic from a different cluster was discarded
* This is caused by using the same `mcast_addr` and `mcast_port` in `UDP` in different clusters
* Solution: use different values for either or both attributes in `UDP` for each separate cluster




Problem #6: TCPPING
-------------------
* TCPPING.initial_hosts doesn't list all cluster members
** If `initial_hosts=A` and we have `{A,B,C}`, then `A` leaves, no new members can join
** Solutions:
*** List all members
*** Use `send_cache_on_join` (`3.6.1` and higher)
*** use `MPING` (if IP multicasting is enabled)

* TCPPING not merging
** Same as above: if we have `initial_hosts=A`, but 2 partitions `{A,B,C}` and `{X,Y,Z}`, then `X` will be able to send a
  message to `A`, but `A` won't be able to respond (it doesn't have `X`'s address) -> no merge


* TCPPING.initial_hosts lists the wrong members
** All members need to be listed with the `bind_addr` they're bound to and the `bind_port` they use

* TCPPING is used but TCP doesn't set `bind_port`
** If `TCP.bind_port` is 0, a random port will be used and we cannot list it in `TCPPING.initial_hosts`



Problem #5: IPv6
----------------
* Running in IPv6 without a correctly configured IPv6 routing table
** By default, the JVM uses IPv6, but the routing table is not configured correctly, or the config uses IPv4
** Solution: look at IPv6 routing or force use of IPv4 (`-Djava.net.preferIPv4Stack=true`)

* Mixing IPv4 and IPv6
** This works with TCP as IPv4 addresses are mapped to IPv4-mapped IPv6 addresses, but this is (IMO) hard to set up correctly

* Wiki: https://developer.jboss.org/wiki/IPv6



Problem #4: JGroups versions
----------------------------
* An old JGroups version is used
** Symptom: a bug that was fixed a long time ago pops up
** Side effect: Bela gets very tired having to waste time on some bug that's already been fixed
** Solution: upgrade to the latest stable JGroups version

* Different JGroups version in the same cluster
** Running different JGroups versions on different nodes might lead to subtle issues, e.g. dropping messages due to
   deserialization issues.
** Solution: run the same version on all cluster nodes

* Old JGroups configuration
** Sometimes, people upgrade to a newer JGroups version, but forget to upgrade their config(s) as well.
** Solution: always use the config template from the JGroups version you upgrade to and apply your specific changes



Problem #3: tinkering with configuration
----------------------------------------
* (The "I'm smarter than Bela" problem)

* Custom configuration files
** A configuration should never be built from the ground up; instead, copy `udp.xml` or `tcp.xml` from the JGroups JAR
   and modify it

* Removing \'unneeded' protocols
** Removing `UNICAST` because the transport is `TCP` (reliable): this won't work as `UNICAST` also performs ordering
** Symptoms: unicast messages can be unordered
** Removing `STABLE` causes OOMEs

* Putting protocols in the wrong place
** A configuration needs to be defined in a certain order; placing protocols in the wrong place almost always causes subtle issues



Problem #2: cluster falls apart
-------------------------------
* Low timeout in `FD` / `FD_ALL`
** GC, high network traffic or exhausted thread pools on the receivers can lead to missing heartbeats, causing members
   to be suspected.
** Symptoms: some members are suspected, excluded and later merged back
** Solution: use high timeouts in heartbeat based failure detection protocols and add `FD_SOCK` / `FD_HOST`

* IGMP Snooping
** Snooping (in the switch) listens on ports for IGMP joins and copies multicast packets for a groups to all joiners of
   that group.
** Buggy firmware code leads to that information getting dropped and multicast packets getting dropped until the
   information has been refreshed.
** Symptoms: multicast groups falls apart every N minutes
** Solution: upgrade switch firmware

* Faulty network card
** Sometimes a faulty NIC randomly drops packets, or drops sent packets but accepts received packets



Problem #1: members don't find each other
-----------------------------------------
* Binding to the loopback interface
** Setting `bind_addr` (in the transport) or system property `jgroups.bind_addr`
   to `127.0.0.1` works when members are running on the same host, but doesn't work across hosts

* Binding to the wrong network interface
** Binding to a VPN tunnel that's down, or `A` binding to `eth0` and `B` binding to `eth1` (different networks)

* Firewalls dropping packets
** Disable the firewall, to see if this helps (e.g. `sudo iptables -F` on Linux).
** If this is the issue, open ports for JGroups (`UDP.bind_port`, `FD_SOCK`, `STATE_SOCK`) and re-enable the firewall

* SELinux
** Restrictive vs. permissive

* Switch dropping packets
** Especially between VLANs. Check the switch configuration

* UDP: time-to-live loo low
** If `UDP` is used, increase the value of `ip_ttl`. See whether packets are received with wireshark / tcpdump




Logging
-------
* JGroups has no runtime dependencies on any logging framework (j.u.l. is used by default)
* At startup, JGroups looks for log4j2, j.u.l. (in this order)
** To force use of JDK logging, even if the log4j2 JARs are present, `-Djgroups.use.jdk_logger=true` can be used
* Custom loggers can be used instead of the ones supported by default. To do this, interface
`CustomLogFactory` has to be implemented:

[source,java]
----
public interface CustomLogFactory {
    Log getLog(Class clazz);
    Log getLog(String category);
}
----

* The implementation needs to return an implementation of `org.jgroups.logging.Log`.
* To force using the custom log implementation, the fully qualified classname of the custom log
  factory has to be provided with `-Djgroups.logging.log_factory_class=com.foo.MyCustomLogger`.
* Ref: http://www.jgroups.org/manual/index.html#Logging


JMX
---
* JGroups exposes attributes and operations of the channel and all protocols via JMX
* Has to be enabled with `-Dcom.sun.management.jmxremote` (or others, ie. remote JMX host:port etc)
* To expose a channel and its attributes via JMX:

[source,java]
----
public static void registerChannel(JChannel channel,String name) {
    JmxConfigurator.registerChannel(channel,
                                    Util.getMBeanServer(),
                                    (name != null? name : "jgroups"),
                                    channel.getClusterName(),
                                    true);
}

// Util.registerChannel((JChannel)channel, channel.getClusterName());

public static void unregisterChannel(Channe channel) {
    JmxConfigurator.unregisterChannel((JChannel)channel,
                                      Util.getMBeanServer(),
                                      channel.getClusterName(());
}
----
* Let's try this out with our ChatDemo


probe.sh
--------
* Probe is a simple program which sends IP multicasts to a given multicast group and port and prints all responses
* Functionality
** Read attributes
** Write attributes
** Invoke operations
** Insert new protocols, remove protocols
* Any application can implement a `ProbeHandler` and expose its attributes and operations
* Probe requests are simple strings that are parsed by cluster nodes
* Probe responses are strings, too
* To enable:

[source,xml]
----
<UDP enable_diagnostics="true"
     diagnostics_addr="xxx"
     diagnostics_port="xxx"
     ...
/>
----
* Let's run ChatDemo and explore the features of probe


Listing all cluster nodes
-------------------------

----
[mac] /Users/bela/workshop/bin$ ./probe.sh

-- sending probe on /224.0.75.75:7500

#1 (149 bytes):
local_addr=A [f91dce0b-a753-987d-9d18-a8e8d86950ee]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:52181
version=3.6.0.Final

#2 (149 bytes):
local_addr=B [9e413b1d-d2f7-eaac-cb67-8eb94b2ba352]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:58998
version=3.6.0.Final


2 responses (2 matches, 0 non matches)
[mac] /Users/bela/workshop/bin$
----


Reading attributes from a protocol
----------------------------------
* Reading the number of sent and received messages and bytes in `UDP`:

----
[mac] /Users/bela/workshop/bin$ ./probe.sh jmx=UDP.num_msgs,num_byt

#1 (246 bytes):
local_addr=A [f91dce0b-a753-987d-9d18-a8e8d86950ee]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:52181
jmx=UDP={num_msgs_received=36, num_msgs_sent=37, num_bytes_received=2325, num_bytes_sent=2470}

version=3.6.0.Final


#2 (246 bytes):
local_addr=B [9e413b1d-d2f7-eaac-cb67-8eb94b2ba352]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:58998
jmx=UDP={num_msgs_received=36, num_msgs_sent=36, num_bytes_received=2372, num_bytes_sent=2325}

version=3.6.0.Final

2 responses (2 matches, 0 non matches)
[mac] /Users/bela/workshop/bin$
----



Setting attributes
------------------
* Changing the log level of `NAKACK2` to `TRACE`:
----
./probe.sh jmx=NAKACK2.level=trace
----
* This allows an admin to change the log level temporarily, and reset it back to `WARN` later



Invoking an operation
---------------------
* Dump the retransmit tables in `NAKACK2`:

----
[mac] /Users/bela/workshop/bin$ ./probe.sh op=NAKACK2.printMessages

#1 (254 bytes):
local_addr=A [f91dce0b-a753-987d-9d18-a8e8d86950ee]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:52181
NAKACK2.printMessages=A:
B: [0 | 0 | 0] (0 elements, 0 missing)
A: [2 | 9 | 9] (0 elements, 0 missing)

#2 (254 bytes):
local_addr=B [9e413b1d-d2f7-eaac-cb67-8eb94b2ba352]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:58998
NAKACK2.printMessages=B:
B: [0 | 0 | 0] (0 elements, 0 missing)
A: [9 | 9 | 9] (0 elements, 0 missing)

[mac] /Users/bela/workshop/bin$
----


Printing the protocol stacks
----------------------------

----
[mac] /Users/bela/workshop/bin$ ./probe.sh print-protocols

#1 (140 bytes):
protocols=UDP
PING
MERGE3
FD_SOCK
FD_ALL
NAKACK2
UNICAST3
STABLE
GMS
UFC
MFC
FRAG2

#2 (140 bytes):
protocols=UDP
PING
MERGE3
FD_SOCK
FD_ALL
NAKACK2
UNICAST3
STABLE
GMS
UFC
MFC
FRAG2

[mac] /Users/bela/workshop/bin$
----


Inserting a protocol at runtime
-------------------------------
* Insert `PRINT_BYTES` above `UDP`:
----
./probe.sh insert-protocol=org.lab.protocols.PRINT_BYTES=above=UDP
----
* Remove `PRINT_BYTES`:
----
./probe.sh remove-protocol=PRINT_BYTES
----
* Works only for stateless protocols
* Use cases
** Temporary TRACE logging to see what's going on in a defective system, then disable TRACE again
** Insert a protocol that extracts relevant information about a cluster, stores this to a file and sends the file to
   support



Use of probe when IP multicasting is not available
--------------------------------------------------
* `probe.sh -addr <address of any member> <diagnostics port (default: 7500)>`
** This asks any member for the addresses of _all members_ and then sends the probe request to all members in turn
* Note that any member can also be queried via simple datagram packets, e.g.:
----
[mac] /Users/bela/workshop/bin$ nc -u 192.168.1.3 7500
uuids
local_addr=A
uuids=2 elements:
B: ca335dc2-f30f-6e11-d13a-b029e3e9e2f1: 192.168.1.3:7801 (300 secs old)
A: 9dd407ae-577d-68b1-4f1e-6623279bb6ed: 192.168.1.3:7800 (31 secs old)

local_addr=A [9dd407ae-577d-68b1-4f1e-6623279bb6ed]
cluster=draw
view=[A|1] (2) [A, B]
physical_addr=192.168.1.3:7800
version=3.6.1.Final
^C
[mac] /Users/bela/workshop/bin$
----

* Ref: http://www.jgroups.org/manual/index.html#Probe

































